<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kaixuan Ji - Academic Homepage</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

<header>
    <div class="container">
        <div class="header-left">
            <img src="profile.png" alt="Kaixuan Ji" class="avatar">
        </div>
        <div class="header-center">
            <h1>Kaixuan Ji</h1>
            <p>Ph.D. Student in Computer Science, University of California, Los Angeles</p>
            <p>
                <a href="https://scholar.google.com/citations?user=example" target="_blank", style="color: antiquewhite;">Google Scholar</a> |
                <a href="mailto:kaixuanji@sc.ucla.edu" target="_blank", style="color: antiquewhite;">Email</a>
                <!-- Email: <a href="mailto:kaixuanji@cs.ucla.edu> -->
            </p>
        </div>
    </div>
</header>

<nav>
  <a href="index.html">About</a>
  <a href="publication.html">Publications</a>
  <a href="Resume_Kaixuan_Ji.pdf">Resume</a>
  <a href="misc.html">Miscellaneous</a>
</nav>

<section id="about">
  <h2>About</h2>
  <p>I am Kaixuan Ji, a third-year Ph.D. student in the Computer Science Department at UCLA, advised by Professor <a href="https://web.cs.ucla.edu/~qgu/">Quanquan Gu</a>. Previously, I finished my undergrade at the department of computer science and technology, Tsinghua University, where I was advised by Professor <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a> and Professor <a href="https://keg.cs.tsinghua.edu.cn/persons/ljz/" >Juanzi Li</a>. My research interest mainly lies on the theory of reinforcement learning and its application in large language model training.</p>
</section>

<section id="research">
  <h2>Selected Publications</h2>
  
  <h4>Bandit and Reinforcement Learning Theory</h4>
  <ul>
    <li> <p> <a href="https://arxiv.org/abs/2305.08359">Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs</a> 
        <br> Kaixuan Ji*, Qingyue Zhao*, Jiafan He, Weitong Zhang, Quanquan Gu, <i>ICLR</i> 2024 </p>
    </li>
    <li> <p> <a href="https://arxiv.org/abs/2502.06051v2">Towards a Sharp Analysis of Offline Policy Learning for f-Divergence-Regularized Contextual Bandits</a> 
        <br> Qingyue Zhao*, Kaixuan Ji*, Heyang Zhao*, Tong Zhang, Quanquan Gu, <i>arXiv:2502.06051</i>. </p>
    </li>
  </ul>

  <h4>Reinforcement Learning for Large Language Models</h4>
  <ul>
    <li> <p> <a href="https://arxiv.org/abs/2402.09401">Reinforcement Learning from Human Feedback with Active Queries</a> 
        <br> Kaixuan Ji*, Jiafan He*, Quanquan Gu, <i>TMLR</i> 2025, <font color="red">Featured Certification</font> </p>
    </li>
    <li> <p> <a href="https://arxiv.org/abs/2405.00675">Self-play Preference Optimization for Language Model Alignment</a> 
        <br>Yue Wu*, Zhiqing Sun*, Huizhuo Yuan*, Kaixuan Ji, Yiming Yang, Quanquan Gu, <i>ICLR</i> 2025, </p>
    </li>
  </ul>

  <br>Previously, I have also worked on the following topic:
  <h4>Efficient Methods for Large Language Model</h4>
  <ul>
    <li> <p> <a href="https://arxiv.org/abs/2110.07602">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</a> 
        <br> Xiao Liu*, Kaixuan Ji*, Yicheng Fu*, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, <i>ACL</i> 2022 </p>
    </li>
  </ul>

</section>

<!-- <section id="contact">
  <h2>Contact</h2>
  <p>Email: <a href="mailto:kaixuanji@ucla.edu">kaixuanji@ucla.edu</a></p>
  <p>GitHub: <a href="https://github.com/kaixuanji" target="_blank">github.com/kaixuanji</a></p>
</section> -->

<footer>
  &copy; 2025 Kaixuan Ji. All rights reserved.
</footer>

</body>
</html>
